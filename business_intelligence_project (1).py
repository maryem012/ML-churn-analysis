# -*- coding: utf-8 -*-
"""business inelligence Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12n4R0stMFVjdfpEE9igoD0HPocqZ7iqx
"""

import pandas as pd

# Correct file path using a raw string
file_path = r'C:\Users\ACER\Desktop\ML\Telecom Churn Rate Dataset.xlsx'

# Load the dataset
df = pd.read_excel(file_path)







df.head(20)

df.info()

df.Churn

churn_count = df['Churn'].value_counts()['Yes']

print("Number of Churns (Churn='Yes'):", churn_count)

churn_count = df['Churn'].value_counts()['No']

print("Number of Churns (Churn='No'):", churn_count)

import numpy as np
import seaborn as sns

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming your DataFrame is named df

# Create a contingency table
contingency_table = pd.crosstab(df['gender'], df['Churn'])

# Plot a bar chart
sns.countplot(x='gender', hue='Churn', data=df)
plt.title('Churn Distribution by Gender')
plt.xlabel('Gender')
plt.ylabel('Count')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming your DataFrame is named df

# Create a contingency table
contingency_table = pd.crosstab(df['MultipleLines'], df['Churn'])

# Plot a bar chart
sns.countplot(x='MultipleLines', hue='Churn', data=df)
plt.title('Churn Distribution by MultipleLines')
plt.xlabel('MultipleLines')
plt.ylabel('Count')
plt.show()

df.TotalCharges

df.describe()

plt.figure(figsize=(8, 6))
sns.countplot(x='Churn', data=df)
plt.title('Distribution of Churn')
plt.show()

df.notnull().sum()

"""**feature selection**


"""

# Select only numeric columns
numeric_df = df.select_dtypes(include=['number'])

# Calculate the correlation matrix
correlation_matrix = numeric_df.corr()

# Display the correlation matrix
print(correlation_matrix)

# Check if 'Churn' is present in the columns
if 'Churn' in numeric_df.columns:
    # If 'Churn' is present, calculate the correlation with 'Churn'
    correlation_with_churn = correlation_matrix['Churn']
    print(correlation_with_churn)
else:
    print("The 'Churn' variable is not present in the DataFrame.")

"""**interpretation**
*  as the number of technical tickets increases, the likelihood of churn also increases.
*   customers with longer tenure are less likely to churn.

*   Higher monthly charges may be associated with a higher likelihood of churn.
*   'SeniorCitizen' and 'numAdminTickets' have weaker correlations with 'Churn'.




"""

import pandas as pd

# Assuming your dataset is stored in a DataFrame called 'df'
# Replace 'your_data' with the actual name of your DataFrame

# Display the data types of each column
data_types = df.dtypes

# Identify columns with object or categorical data types
categorical_columns = data_types[data_types == 'object'].index.tolist()

# Display the list of categorical columns
print("Categorical Columns:", categorical_columns)

"""##feature selection with lasso"""

from sklearn.linear_model import LassoCV
from sklearn.model_selection import train_test_split
import pandas as pd

# Assuming your dataset is stored in a DataFrame called 'df'
# Replace 'your_data' with the actual name of your DataFrame

# One-hot encode categorical variables
X_encoded = pd.get_dummies(df, columns=['customerID', 'gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'TotalCharges'])

# Separate features (X) and target variable (y)
X = X_encoded.drop('Churn', axis=1)  # Use the encoded features excluding the target variable
y = X_encoded['Churn'].map({'No': 0, 'Yes': 1})  # Convert 'Churn' to numerical format

# Select only numeric columns
numeric_df = X.select_dtypes(include=['number'])

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(numeric_df, y, test_size=0.2, random_state=42)

# Initialize the LassoCV model
lasso_model = LassoCV(alphas=[0.01, 0.1, 1.0, 10.0], cv=5)

# Fit the model on the training data
lasso_model.fit(X_train, y_train)

# Display the selected features based on non-zero coefficients
selected_features = numeric_df.columns[lasso_model.coef_ != 0]
print("Selected Features:", selected_features)

# Evaluate the model on the testing set
accuracy = lasso_model.score(X_test, y_test)
print("Model Accuracy:", accuracy)

"""### Random forest"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import train_test_split
import pandas as pd
import joblib
import os


# Define the directory path where you want to save the model
dir_path = r'C:\Users\ACER\Desktop\ML'

# Check if the directory exists, and create it if it does not
if not os.path.exists(dir_path):
    os.makedirs(dir_path)
# Assuming your dataset is stored in a DataFrame called 'df'
# Replace 'your_data' with the actual name of your DataFrame

# One-hot encode categorical variables
df_encoded = pd.get_dummies(df, columns=['InternetService', 'OnlineSecurity', 'OnlineBackup',
                                          'DeviceProtection', 'TechSupport', 'Contract',
                                          'PaperlessBilling', 'PaymentMethod'])

# Separate features (X) and target variable (y)
X = df_encoded.drop('Churn', axis=1)
y = df_encoded['Churn']

# List of selected features from LASSO
selected_features = ['tenure', 'MonthlyCharges', 'numTechTickets',
                     'InternetService_Fiber optic', 'OnlineSecurity_No',
                     'OnlineBackup_No', 'DeviceProtection_No',
                     'TechSupport_No', 'Contract_Month-to-month',
                     'Contract_One year', 'PaperlessBilling_No',
                     'PaperlessBilling_Yes', 'PaymentMethod_Electronic check']

# Create the feature matrix (X) using the selected features
X_selected = X[selected_features]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)

# Initialize and train a RandomForestClassifier
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print("Model Accuracy:", accuracy)

# Display additional classification metrics
print("Classification Report:")
print(classification_report(y_test, y_pred))
# Define the full path for saving the model

model_path = os.path.join(dir_path, 'random_forest_model.pkl')

# Save the model to disk
joblib.dump(model, model_path)
"""## Decision Tree Classifier"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import pandas as pd

# Assuming your dataset is stored in a DataFrame called 'df'
# Replace 'your_data' with the actual name of your DataFrame

# One-hot encode categorical variables
df_encoded = pd.get_dummies(df, columns=['InternetService', 'OnlineSecurity', 'OnlineBackup',
                                          'DeviceProtection', 'TechSupport', 'Contract',
                                          'PaperlessBilling', 'PaymentMethod'])

# Extract selected features for decision tree
selected_features = ['tenure', 'MonthlyCharges', 'numTechTickets',
                     'InternetService_Fiber optic', 'OnlineSecurity_No',
                     'OnlineBackup_No', 'DeviceProtection_No',
                     'TechSupport_No', 'Contract_Month-to-month',
                     'Contract_One year', 'PaperlessBilling_No',
                     'PaperlessBilling_Yes', 'PaymentMethod_Electronic check']

# Create a new DataFrame with selected features
X = df_encoded[selected_features]
y = df_encoded['Churn']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Decision Tree model
decision_tree_model = DecisionTreeClassifier(random_state=42)

# Fit the model on the training data
decision_tree_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = decision_tree_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print("Model Accuracy:", accuracy)

# Display additional classification metrics
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

"""##kmeans

"""

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Assuming your dataset is stored in a DataFrame called 'df'
# Replace 'your_data' with the actual name of your DataFrame

# One-hot encode categorical variables
df_encoded = pd.get_dummies(df, columns=['InternetService', 'OnlineSecurity', 'OnlineBackup',
                                          'DeviceProtection', 'TechSupport', 'Contract',
                                          'PaperlessBilling', 'PaymentMethod'])

# Extract selected features for clustering
selected_features = ['tenure', 'MonthlyCharges', 'numTechTickets',
                     'InternetService_Fiber optic', 'OnlineSecurity_No',
                     'OnlineBackup_No', 'DeviceProtection_No',
                     'TechSupport_No', 'Contract_Month-to-month',
                     'Contract_One year', 'PaperlessBilling_No',
                     'PaperlessBilling_Yes', 'PaymentMethod_Electronic check']

# Create a new DataFrame with selected features
data_for_clustering = df_encoded[selected_features]

# Standardize numerical features
scaler = StandardScaler()
data_for_clustering_scaled = scaler.fit_transform(data_for_clustering)

# Choose the number of clusters (k)
k = 3  # You can adjust this based on your requirements

# Apply k-means clustering
kmeans = KMeans(n_clusters=k, random_state=42)
df_encoded['Cluster'] = kmeans.fit_predict(data_for_clustering_scaled)

# Visualize the clusters (assuming 2D for simplicity, adjust as needed)
plt.scatter(data_for_clustering_scaled[:, 0], data_for_clustering_scaled[:, 1], c=df_encoded['Cluster'], cmap='viridis')
plt.title('K-Means Clustering')
plt.xlabel('Feature 1 (Standardized)')
plt.ylabel('Feature 2 (Standardized)')
plt.show()

# Display the cluster centers
cluster_centers = pd.DataFrame(scaler.inverse_transform(kmeans.cluster_centers_), columns=data_for_clustering.columns)
print("Cluster Centers:")
print(cluster_centers)

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Assuming your dataset is stored in a DataFrame called 'df'
# Replace 'your_data' with the actual name of your DataFrame

# One-hot encode categorical variables
df_encoded = pd.get_dummies(df, columns=['InternetService', 'OnlineSecurity', 'OnlineBackup',
                                          'DeviceProtection', 'TechSupport', 'Contract',
                                          'PaperlessBilling', 'PaymentMethod'])

# Extract selected features for clustering
selected_features = ['tenure', 'MonthlyCharges', 'numTechTickets',
                     'InternetService_Fiber optic', 'OnlineSecurity_No',
                     'OnlineBackup_No', 'DeviceProtection_No',
                     'TechSupport_No', 'Contract_Month-to-month',
                     'Contract_One year', 'PaperlessBilling_No',
                     'PaperlessBilling_Yes', 'PaymentMethod_Electronic check']

# Create a new DataFrame with selected features
data_for_clustering = df_encoded[selected_features]

# Standardize numerical features
scaler = StandardScaler()
data_for_clustering_scaled = scaler.fit_transform(data_for_clustering)

# Choose a range of k values
k_values = range(1, 11)

# Apply k-means clustering for each k
inertia_values = []
for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(data_for_clustering_scaled)
    inertia_values.append(kmeans.inertia_)

# Plot the elbow curve
plt.plot(k_values, inertia_values, marker='o')
plt.title('Elbow Method for Optimal k')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia (Within-Cluster Sum of Squares)')
plt.show()

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler
import joblib

# Assuming your dataset is stored in a DataFrame called 'df'
# Replace 'your_data' with the actual name of your DataFrame

# One-hot encode categorical variables
df_encoded = pd.get_dummies(df, columns=['InternetService', 'OnlineSecurity', 'OnlineBackup',
                                          'DeviceProtection', 'TechSupport', 'Contract',
                                          'PaperlessBilling', 'PaymentMethod'])

# Extract selected features for clustering
selected_features = ['tenure', 'MonthlyCharges', 'numTechTickets',
                     'InternetService_Fiber optic', 'OnlineSecurity_No',
                     'OnlineBackup_No', 'DeviceProtection_No',
                     'TechSupport_No', 'Contract_Month-to-month',
                     'Contract_One year', 'PaperlessBilling_No',
                     'PaperlessBilling_Yes', 'PaymentMethod_Electronic check']

# Create a new DataFrame with selected features
data_for_clustering = df_encoded[selected_features]

# Standardize numerical features
scaler = StandardScaler()
data_for_clustering_scaled = scaler.fit_transform(data_for_clustering)

# Choose a range of k values
k_values = range(2, 11)  # Start with at least 2 clusters

# Calculate silhouette scores for each value of k
silhouette_scores = []

for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(data_for_clustering_scaled)
    silhouette_avg = silhouette_score(data_for_clustering_scaled, cluster_labels)
    silhouette_scores.append(silhouette_avg)

# Plot the silhouette scores
plt.plot(k_values, silhouette_scores, marker='o')
plt.title('Silhouette Score for Optimal k')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Silhouette Score')
plt.show()

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler
import pandas as pd

# Assuming your dataset is stored in a DataFrame called 'df'
# Replace 'your_data' with the actual name of your DataFrame

# One-hot encode categorical variables
df_encoded = pd.get_dummies(df, columns=['InternetService', 'OnlineSecurity', 'OnlineBackup',
                                          'DeviceProtection', 'TechSupport', 'Contract',
                                          'PaperlessBilling', 'PaymentMethod'])

# Extract selected features for clustering
selected_features = ['tenure', 'MonthlyCharges', 'numTechTickets',
                     'InternetService_Fiber optic', 'OnlineSecurity_No',
                     'OnlineBackup_No', 'DeviceProtection_No',
                     'TechSupport_No', 'Contract_Month-to-month',
                     'Contract_One year', 'PaperlessBilling_No',
                     'PaperlessBilling_Yes', 'PaymentMethod_Electronic check']

# Create a new DataFrame with selected features
data_for_clustering = df_encoded[selected_features]

# Standardize numerical features
scaler = StandardScaler()
data_for_clustering_scaled = scaler.fit_transform(data_for_clustering)

# Apply K-means clustering with a chosen k value
k = 2  # You can adjust this based on the elbow method or other criteria
kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
labels = kmeans.fit_predict(data_for_clustering_scaled)

# Calculate the silhouette score
silhouette_avg = silhouette_score(data_for_clustering_scaled, labels)
print(f"Silhouette Score: {silhouette_avg}")



"""True Positives (TP): 246
True Negatives (TN): 902
False Positives (FP): 134
False Negatives (FN): 127

## PCA
"""

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import pandas as pd
import matplotlib.pyplot as plt

# Assuming your dataset is stored in a DataFrame called 'df'
# Replace 'your_data' with the actual name of your DataFrame

# One-hot encode categorical variables
df_encoded = pd.get_dummies(df, columns=['InternetService', 'OnlineSecurity', 'OnlineBackup',
                                          'DeviceProtection', 'TechSupport', 'Contract',
                                          'PaperlessBilling', 'PaymentMethod'])

# Extract selected features for PCA
selected_features = ['tenure', 'MonthlyCharges', 'numTechTickets',
                     'InternetService_Fiber optic', 'OnlineSecurity_No',
                     'OnlineBackup_No', 'DeviceProtection_No',
                     'TechSupport_No', 'Contract_Month-to-month',
                     'Contract_One year', 'PaperlessBilling_No',
                     'PaperlessBilling_Yes', 'PaymentMethod_Electronic check']

# Create a new DataFrame with selected features
data_for_pca = df_encoded[selected_features]

# Standardize numerical features
scaler = StandardScaler()
data_for_pca_scaled = scaler.fit_transform(data_for_pca)

# Apply PCA
pca = PCA()
principal_components = pca.fit_transform(data_for_pca_scaled)

# Variance explained by each principal component
explained_variance_ratio = pca.explained_variance_ratio_

# Plot the cumulative explained variance
cumulative_explained_variance = explained_variance_ratio.cumsum()
plt.plot(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, marker='o')
plt.title('Cumulative Explained Variance by Principal Components')
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Explained Variance')
plt.show()

# Set a threshold for cumulative explained variance
threshold = 0.95

# Determine the optimal number of principal components
optimal_components = sum(cumulative_explained_variance < threshold) + 1
print(f"Optimal Number of Components to Retain {threshold}% Variance: {optimal_components}")

# Extract the principal components based on the optimal number
optimal_principal_components = principal_components[:, :optimal_components]

# If you want to use the optimal components for further analysis, you can use 'optimal_principal_components'

principal_components_df = pd.DataFrame(pca.components_, columns=data_for_pca.columns)

import pandas as pd

# Assuming 'pca' is your trained PCA model
# Assuming 'data_for_pca' is your original data

# Extract the loadings for the retained principal components
loadings = pd.DataFrame(pca.components_[:optimal_components, :], columns=data_for_pca.columns)

# Display the feature loadings
print("Feature Loadings:")
print(loadings)
# Determine the optimal number of principal components
optimal_components = sum(cumulative_explained_variance < threshold) + 1
print(f"Optimal Number of Components to Retain {threshold}% Variance: {optimal_components}")

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np

from joblib import dump, load  # Use this line for newer versions of joblib and sklearn

# Assuming your dataset is stored in a DataFrame called 'df'
# Replace 'your_data' with the actual name of your DataFrame

# One-hot encode categorical variables
df_encoded = pd.get_dummies(df, columns=['InternetService', 'OnlineSecurity', 'OnlineBackup',
                                          'DeviceProtection', 'TechSupport', 'Contract',
                                          'PaperlessBilling', 'PaymentMethod'])

# Extract selected features for PCA
selected_features = ['tenure', 'MonthlyCharges', 'numTechTickets',
                     'InternetService_Fiber optic', 'OnlineSecurity_No',
                     'OnlineBackup_No', 'DeviceProtection_No',
                     'TechSupport_No', 'Contract_Month-to-month',
                     'Contract_One year', 'PaperlessBilling_No',
                     'PaperlessBilling_Yes', 'PaymentMethod_Electronic check']

# Create a new DataFrame with selected features
data_for_pca = df_encoded[selected_features]

# Standardize numerical features
scaler = StandardScaler()
data_for_pca_scaled = scaler.fit_transform(data_for_pca)

# Apply PCA
pca = PCA()
optimal_components = 10  # Use the optimal number of components determined earlier
reduced_data = pca.fit_transform(data_for_pca_scaled)[:, :optimal_components]

# Reconstruct data to calculate reconstruction error
reconstructed_data = reduced_data @ pca.components_[:optimal_components, :]
reconstruction_error = np.sum((data_for_pca_scaled - reconstructed_data) ** 2, axis=1)

# Set a threshold for the reconstruction error
threshold = 3.0  # Adjust the threshold based on your specific requirements

# Identify outliers
outliers = df[reconstruction_error > threshold]

# Display identified outliers
print("Identified Outliers:")
print(outliers)

"""##Naive bayes

"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import StandardScaler

# Replace ' ' with NaN in the entire DataFrame
df.replace(' ', np.nan, inplace=True)

# Drop rows with NaN values
df.dropna(inplace=True)

# One-hot encode categorical variables
df_encoded = pd.get_dummies(df, columns=['InternetService', 'OnlineSecurity', 'OnlineBackup',
                                          'DeviceProtection', 'TechSupport', 'Contract',
                                          'PaperlessBilling', 'PaymentMethod'])

# Separate features (X) and target variable (y)
X = df_encoded.drop('Churn', axis=1)
y = df_encoded['Churn']

# List of selected features from PCA or other methods
selected_features = ['tenure', 'MonthlyCharges', 'TotalCharges', 'numTechTickets']

# Create the feature matrix (X) using the selected features
X_selected = X[selected_features]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)

# Standardize the features (important for Gaussian Naive Bayes)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Standardize the features (important for Gaussian Naive Bayes)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize Gaussian Naive Bayes classifier
nb_classifier = GaussianNB()

# Train the classifier
nb_classifier.fit(X_train_scaled, y_train)

# Make predictions on the test set
y_pred = nb_classifier.predict(X_test_scaled)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print("Model Accuracy:", accuracy)

# Display additional classification metrics
print("Classification Report:")
print(classification_report(y_test, y_pred))

"""use kmeans and pca to see the best features

"""

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt


# One-hot encode categorical variables
df_encoded = pd.get_dummies(df, columns=['InternetService', 'OnlineSecurity', 'OnlineBackup',
                                          'DeviceProtection', 'TechSupport', 'Contract',
                                          'PaperlessBilling', 'PaymentMethod'])

# Extract selected features for clustering
selected_features = ['tenure', 'MonthlyCharges', 'TotalCharges', 'numTechTickets']

# Create a new DataFrame with selected features
data_for_clustering = df_encoded[selected_features]

# Standardize numerical features
scaler = StandardScaler()
data_for_clustering_scaled = scaler.fit_transform(data_for_clustering)

# Apply KMeans clustering
kmeans = KMeans(n_clusters=2, random_state=42)  # Set the desired number of clusters
cluster_labels = kmeans.fit_predict(data_for_clustering_scaled)

# Apply PCA for dimensionality reduction
pca = PCA(n_components=2)
data_pca = pca.fit_transform(data_for_clustering_scaled)

# Create a DataFrame with PCA results and cluster labels
df_pca = pd.DataFrame(data_pca, columns=['PC1', 'PC2'])
df_pca['Cluster'] = cluster_labels
df_pca['Churn'] = df['Churn']  # Add the 'Churn' column for color differentiation

# Visualize the clusters with more detail
plt.figure(figsize=(12, 8))
sns.scatterplot(x='PC1', y='PC2', hue='Cluster', style='Churn', data=df_pca, palette='viridis', markers=['o', 's'], alpha=0.7)
plt.title('Clusters in Reduced-Dimensional Space (PCA)')
plt.xlabel('Principal Component 1 (PC1)')
plt.ylabel('Principal Component 2 (PC2)')
plt.legend(title='Cluster', loc='upper right')
plt.show()
print("Explained variance by component: ", pca.explained_variance_ratio_)

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns

# Load and prepare your dataset
# df = pd.read_csv('your_data.csv')

# One-hot encode categorical variables
df_encoded = pd.get_dummies(df, columns=['InternetService', 'OnlineSecurity', 'OnlineBackup',
                                         'DeviceProtection', 'TechSupport', 'Contract',
                                         'PaperlessBilling', 'PaymentMethod'])

# Extract selected features for clustering
selected_features = ['tenure', 'MonthlyCharges', 'TotalCharges', 'numTechTickets',
                     'InternetService_Fiber optic', 'OnlineSecurity_No',
                     'OnlineBackup_No', 'DeviceProtection_No', 'TechSupport_No',
                     'Contract_Month-to-month', 'PaperlessBilling_Yes',
                     'PaymentMethod_Electronic check']

# Create a new DataFrame with selected features
data_for_clustering = df_encoded[selected_features]

# Standardize numerical features
scaler = StandardScaler()
data_for_clustering_scaled = scaler.fit_transform(data_for_clustering)

# Choose a range of k values
k_values = range(2, 11)  # Start with at least 2 clusters

# Calculate silhouette scores for each value of k
silhouette_scores = []
for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(data_for_clustering_scaled)
    silhouette_avg = silhouette_score(data_for_clustering_scaled, cluster_labels)
    silhouette_scores.append(silhouette_avg)

# Determine the best number of clusters
optimal_k = k_values[silhouette_scores.index(max(silhouette_scores))]
print("Optimal number of clusters: ", optimal_k)

# Plot the silhouette scores
plt.figure(figsize=(10, 6))
plt.plot(k_values, silhouette_scores, marker='o')
plt.title('Silhouette Score for Optimal k')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Silhouette Score')
plt.grid(True)
plt.show()

# Perform KMeans clustering with the optimal number of clusters
kmeans_optimal = KMeans(n_clusters=optimal_k, random_state=42)
cluster_labels_optimal = kmeans_optimal.fit_predict(data_for_clustering_scaled)

# Apply PCA for dimensionality reduction to visualize
pca = PCA(n_components=2)
data_pca = pca.fit_transform(data_for_clustering_scaled)

# Create a DataFrame with PCA results and cluster labels
df_pca = pd.DataFrame(data_pca, columns=['PC1', 'PC2'])
df_pca['Cluster'] = cluster_labels_optimal
df_pca['Churn'] = df['Churn']  # Add the 'Churn' column for color differentiation

# Visualize the clusters with more detail
plt.figure(figsize=(12, 8))
sns.scatterplot(x='PC1', y='PC2', hue='Cluster', style='Churn', data=df_pca, palette='viridis', markers=['o', 's'], alpha=0.7)
plt.title('Clusters in Reduced-Dimensional Space (PCA) with Optimal k')
plt.xlabel('Principal Component 1 (PC1)')
plt.ylabel('Principal Component 2 (PC2)')
plt.legend(title='Cluster', loc='upper right')
plt.show()
df_encoded['Cluster'] = cluster_labels_optimal



# Print explained variance by PCA components
print("Explained variance by PCA components: ", pca.explained_variance_ratio_)

"""# Churn Prediction Model Comparison

In this notebook, we compare several models to determine the best performer for predicting customer churn. The models evaluated include KMeans clustering, Decision Tree Classifier, and Naive Bayes. We assess their performance based on various metrics such as accuracy, precision, recall, F1-score, and the silhouette score for clustering.

## Model Evaluation Summary

We will explore the performance of each model in detail. The following key metrics have been used to evaluate each model:
- **Accuracy**: Overall correctness of the model
- **Precision and Recall**: Measures of the model's accuracy in predicting positive and negative classes
- **F1-Score**: Harmonic mean of precision and recall
- **Silhouette Score** (for KMeans): Measures the quality of clusters formed

### KMeans Clustering Analysis

KMeans clustering was used to segment the customer base into distinct groups. Here's how the model performed:

- **Silhouette Score**: 0.232

A silhouette score of 0.232 suggests moderate clustering quality, indicating potential overlap between clusters. This metric helps in understanding the effectiveness of the clustering approach.

### Decision Tree Classifier Performance

The Decision Tree model has shown strong performance across several metrics:

- **Accuracy**: 81.48%
- **Precision**: 88% (No), 65% (Yes)
- **Recall**: 87% (No), 66% (Yes)
- **F1-Score**: 87% (No), 65% (Yes)

### Discussion of Random Forest Results

The Random Forest model demonstrated excellent performance metrics:
- **Accuracy**: 85.59%
- **Precision**: 90% (No), 74% (Yes)
- **Recall**: 91% (No), 71% (Yes)
- **F1-Score**: 90% (No), 72% (Yes)

These results suggest that the Random Forest model not only predicts the majority class (No Churn) very well but also performs commendably on the minority class (Churn), which is crucial for churn prediction models. The balance between recall and precision for the 'Churn' category makes it a reliable choice in scenarios where both the identification of true positives and the reduction of false positives are important.

### Naive Bayes Classifier Performance

Naive Bayes model metrics are as follows:

- **Accuracy**: 78.89%
- **Precision**: 79% (No), 74% (Yes)
- **Recall**: 96% (No), 32% (Yes)
- **F1-Score**: 87% (No), 44% (Yes)

Although not explicitly shown, the confusion matrix for Naive Bayes suggests a high recall for the "No" class and lower performance for the "Yes" class.

## Comparison and Final Recommendation

After reviewing the metrics:
## Model Comparison and Recommendations

The following summary of model performances helps in selecting the best model for churn prediction:

- **Random Forest**: Shows the highest accuracy and robust performance across all metrics.
- **Decision Tree**: Offers good interpretability but slightly lower performance compared to Random Forest.
- **Naive Bayes**: Struggles with the recall for the "Yes" class, making it less suitable for this particular problem.
- **KMeans**: Provides insights into data grouping but does not directly predict churn.

**Recommendation**: Based on the analysis, the Random Forest model is recommended for operational use due to its high accuracy and balanced performance across different metrics. This model is suitable for developing effective churn prevention strategies.

### Conclusion

This analysis has identified the Random Forest model as the most effective in predicting customer churn, combining high accuracy with a good balance of precision and recall. Implementing this model can enhance customer retention efforts and improve predictive accuracy in real-world scenarios.
"""